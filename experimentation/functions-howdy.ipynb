{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import os\n",
    "import re\n",
    "import faiss\n",
    "from unidecode import unidecode\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from openai import Client\n",
    "from typing import List\n",
    "from confighowdy import PROCESSED_FILE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions are ordered by appearance, except for prompt-related functions, which are placed at the beginning for easy access and modification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_profile_prompt(input_dataframe: pd.DataFrame) -> str:\n",
    "    \"\"\"\n",
    "    Generates a profile prompt string using specific information from the input DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        input_dataframe (pd.DataFrame): A DataFrame containing columns such as 'Name', \n",
    "                                        'Partner', 'Industry', 'Technologies', and 'Processed_CV'.\n",
    "                                        \n",
    "    Returns:\n",
    "        str: A formatted string that describes the person's profile based on the DataFrame content.\n",
    "    \"\"\"\n",
    "    profile_prompt_string = f\"\"\" {input_dataframe[\"Name\"]} works in \n",
    "    {input_dataframe[\"Partner\"]}, a company of {input_dataframe[\"Industry\"]}\n",
    "    which works with these technologies {input_dataframe[\"Technologies\"]}.\n",
    "    He has expertise in {input_dataframe[\"Processed_CV\"]}\n",
    "    \"\"\"\n",
    "    \n",
    "    return profile_prompt_string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(input_cv: str, prompt_version: int) -> str:\n",
    "    \"\"\"\n",
    "    Generates a prompt based on the input CV and the selected prompt version.\n",
    "\n",
    "    Parameters:\n",
    "    input_cv (str): The curriculum vitae provided as input.\n",
    "    prompt_version (int): The version of the prompt to be created. \n",
    "                    Version 1 extracts detailed information, while Version 2 provides a brief specialization list.\n",
    "\n",
    "    Returns:\n",
    "    str: A formatted prompt string with instructions based on the chosen version.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Version 1 prompt: Extracts specific information from the CV.\n",
    "    if prompt_version == 1:\n",
    "        content = f\"\"\"I want you to read the following CV: {input_cv}\n",
    "\n",
    "        From this, please extract the following information (dates are not important):\n",
    "        1 - Age, seniority, and current job\n",
    "        2 - Education\n",
    "        3 - Previous jobs\n",
    "        4 - Areas of expertise\n",
    "        5 - General knowledge\n",
    "        6 - A TECHNICAL evaluation of one paragraph regarding the individual\n",
    "        7 - Possible gaps in their education\n",
    "\n",
    "        Do not add any additional sections.\n",
    "        \"\"\"\n",
    "    \n",
    "    # Version 2 prompt: Requests a brief list of five areas of specialization from the CV.\n",
    "    if prompt_version == 2:\n",
    "        content = f\"\"\"I want you to read the following CV: {input_cv}\n",
    "\n",
    "        Provide a brief list of five areas of specialization. No need to describe the specifics of each area.\n",
    "        \"\"\"\n",
    "    \n",
    "    # Returns the constructed prompt based on the selected version.\n",
    "    return content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDF processsing functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_all_pdfs(folder: str) -> dict:\n",
    "    \"\"\"\n",
    "    Processes all PDF files in the specified folder, extracts their text content, and stores the content\n",
    "    in a dictionary where the keys are the filenames and the values are the extracted text.\n",
    "\n",
    "    Parameters:\n",
    "    folder (str): The path to the folder containing the PDF files.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary where the keys are the PDF filenames and the values are the extracted text content of each PDF.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Dictionary to store the content of each PDF file\n",
    "    cvs_dict = {}\n",
    "\n",
    "    # Iterate over all files in the specified folder\n",
    "    for file in os.listdir(folder):\n",
    "        # Check if the file has a .pdf extension\n",
    "        if file.endswith('.pdf'):\n",
    "            path = os.path.join(folder, file)\n",
    "            \n",
    "            # Open and read the PDF file\n",
    "            with open(path, 'rb') as pdf_file:\n",
    "                reader = PyPDF2.PdfReader(pdf_file)\n",
    "                all_text = \"\"\n",
    "                \n",
    "                # Extract text from all pages of the PDF\n",
    "                for page_num in range(len(reader.pages)):\n",
    "                    page = reader.pages[page_num]\n",
    "                    all_text += page.extract_text()\n",
    "                \n",
    "                # Store the extracted text in the dictionary with the filename as the key\n",
    "                cvs_dict[file] = all_text\n",
    "\n",
    "    # Return the dictionary with all the extracted text\n",
    "    return cvs_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process CVs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_cv(client: Client, query_gpt: str) -> str:\n",
    "    \"\"\"\n",
    "    Sends a query to a GPT-based chat model and returns a summarized response based on the input CV.\n",
    "\n",
    "    Parameters:\n",
    "    client: The client object used to make API calls to the GPT chat model.\n",
    "    query_gpt (str): The prompt or query to be sent to the GPT model for processing.\n",
    "\n",
    "    Returns:\n",
    "    str: The content of the response generated by the GPT model.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Make the call to the GPT chat model\n",
    "    cv_helper_chat = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": query_gpt,\n",
    "            }\n",
    "        ],\n",
    "        model=\"gpt-4o\",\n",
    "    )\n",
    "\n",
    "    # Extract the content of the response message\n",
    "    response_content = cv_helper_chat.choices[0].message.content\n",
    "\n",
    "    return response_content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_partners_cvs(cvs_dict: dict, save: bool = True) -> dict:\n",
    "    \"\"\"\n",
    "    Processes a dictionary of CVs, summarizes each one, and optionally saves the result.\n",
    "\n",
    "    Args:\n",
    "        cvs_dict (dict): A dictionary where keys are partner names and values are their corresponding CVs.\n",
    "        save (bool): A flag indicating whether to save the processed CVs to a file. Default is True.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are partner names and values are the summarized CVs.\n",
    "    \"\"\"\n",
    "    \n",
    "    cvs_dict_keys = list(cvs_dict.keys())  # Extracting the keys from the input dictionary\n",
    "    keys = cvs_dict_keys\n",
    "    dict_size = len(keys)\n",
    "    all_cvs_processed = {}  # Dictionary to store the processed CVs\n",
    "\n",
    "    for i in range(dict_size):\n",
    "        # Choose prompt version and CV\n",
    "        prompt_version = 2\n",
    "        name = cvs_dict_keys[i]\n",
    "        input_cv = cvs_dict[name]\n",
    "\n",
    "        # Create prompt for GPT\n",
    "        query_gpt = create_prompt(input_cv, prompt_version)\n",
    "\n",
    "        # Append to the list of all processed CVs\n",
    "        output_cv = summarize_cv(client, query_gpt)\n",
    "        all_cvs_processed[name] = output_cv\n",
    "    \n",
    "    # Convert the processed CVs dictionary into a DataFrame\n",
    "    df_cvs_processed = pd.DataFrame.from_dict(all_cvs_processed, orient='index', columns=['Processed_CV'])\n",
    "\n",
    "    if save:\n",
    "        # Save the DataFrame as a CSV file\n",
    "        df_cvs_processed.to_csv(PROCESSED_FILE)\n",
    "    \n",
    "    return all_cvs_processed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def organize_dataframe(df_cvs_processed: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Organizes and processes a DataFrame by renaming columns, modifying entries, and applying transformations.\n",
    "\n",
    "    Args:\n",
    "        df_cvs_processed (pd.DataFrame): The input DataFrame containing CV data that needs processing.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The processed DataFrame with organized and cleaned data.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Rename the 'Unnamed: 0' column to 'Name'\n",
    "    df_cvs_processed.rename(columns={'Unnamed: 0': 'Name'}, inplace=True)\n",
    "\n",
    "    # Replace underscores with spaces in the 'Name' column entries\n",
    "    df_cvs_processed['Name'] = df_cvs_processed['Name'].str.replace('_', ' ')\n",
    "\n",
    "    # Remove the last 11 characters (\"Resume.pdf\") from entries in the 'Name' column\n",
    "    df_cvs_processed['Name'] = df_cvs_processed['Name'].str[:-11]\n",
    "\n",
    "    # Remove accents from the 'Name' column\n",
    "    df_cvs_processed['Name'] = df_cvs_processed['Name'].apply(unidecode)\n",
    "\n",
    "    # Apply the 'extract_sections' function to the 'Processed_CV' column and create the new column 'Processed_CV_list'\n",
    "    df_cvs_processed['Processed_CV_list'] = df_cvs_processed['Processed_CV'].apply(extract_sections)\n",
    "    \n",
    "    return df_cvs_processed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_sections(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Extracts sections of text that are delimited by single digits followed by text, until the next digit or the end of the string.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The input string from which to extract sections.\n",
    "    \n",
    "    Returns:\n",
    "        List[str]: A list of extracted and cleaned text sections.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the regex pattern to find sections of text between single digits\n",
    "    pattern = r\"\\b\\d\\b\\s*(.*?)(?=\\b\\d\\b|$)\"\n",
    "    \n",
    "    # Extract matches from the text using the regex pattern\n",
    "    matches = re.findall(pattern, text, re.DOTALL)\n",
    "    \n",
    "    # Clean the matches by removing any trailing '.' and '\\n'\n",
    "    cleaned_matches = [match.strip('. \\n') for match in matches]\n",
    "    \n",
    "    return cleaned_matches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partner database processing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_partner_information(file_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Loads Excel sheets, merges two specific sheets on the 'Partner' column, and processes the 'Name' column.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the Excel file containing multiple sheets.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A merged DataFrame that combines information from the 'Developers' and 'Partners' sheets.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load all sheets into a dictionary of DataFrames\n",
    "    all_sheets = pd.read_excel(file_path, sheet_name=None)\n",
    "\n",
    "    # Access specific sheets\n",
    "    df_sheet1 = all_sheets[\"Developers\"]\n",
    "    df_sheet2 = all_sheets[\"Partners\"]\n",
    "\n",
    "    # Merge the two sheets on the 'Partner' column\n",
    "    merged_df = pd.merge(df_sheet1, df_sheet2, on=\"Partner\", how=\"inner\")\n",
    "\n",
    "    # Normalize the 'Name' column by removing accents\n",
    "    merged_df['Name'] = merged_df['Name'].apply(unidecode)\n",
    "\n",
    "    return merged_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding and similarity related functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding(client: Client, cv_to_embed: str) -> list:\n",
    "    \"\"\"\n",
    "    Generates embeddings for a given text (in this case, a CV) using the specified client and model.\n",
    "\n",
    "    Parameters:\n",
    "    client: The client object used to interact with the OpenAI API.\n",
    "    cv_to_embed (str): The text (CV) that you want to convert into embeddings.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of embeddings (floating point numbers) representing the semantic representation of the input text.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Call the method to generate embeddings using the specified model\n",
    "    embedding_response = client.embeddings.create(\n",
    "        input=cv_to_embed,\n",
    "        model=\"text-embedding-ada-002\"  # Recommended model for embeddings\n",
    "    )\n",
    "\n",
    "    # Extract the embeddings from the response\n",
    "    embeddings = embedding_response.data[0].embedding\n",
    "\n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_and_normalize_embeddings(client: Client, profile_df: pd.DataFrame, save: bool = True) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generates embeddings for a list of processed CVs, normalizes them, and returns the normalized embedding matrix.\n",
    "\n",
    "    Parameters:\n",
    "    client: The OpenAI client object used to generate embeddings via the `create_embedding` function.\n",
    "    all_cvs_processed (list): A list of processed CVs, where each CV is a string.\n",
    "    n_cvs (int): The total number of CVs to process.\n",
    "\n",
    "    Returns:\n",
    "    np.ndarray: A 2D array of normalized embeddings (with shape `(n_cvs, embedding_dimension)`).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Initialize an empty list to store the embeddings\n",
    "    embedding_matrix = []\n",
    "\n",
    "    # Step 2: Loop through the number of CVs and generate embeddings for each\n",
    "    for n in range(len(profile_df)):\n",
    "        profile_prompt = create_profile_prompt(profile_df.iloc[n])\n",
    "        cv_embedded = create_embedding(client, profile_prompt)  # Create embedding for each CV\n",
    "        embedding_matrix.append(cv_embedded)  # Append the embedding to the list\n",
    "\n",
    "    # Step 3: Convert the list of embeddings to a NumPy array of type float32\n",
    "    embedding_matrix_array = np.array(embedding_matrix).astype('float32')\n",
    "    \n",
    "    # Step 4: Normalize the embeddings (just in case, to ensure they have unit norm)\n",
    "    normalized_embeddings = normalize(embedding_matrix_array)\n",
    "    \n",
    "    if save:\n",
    "        np.save(\"normalized_embeddings.npy\", normalized_embeddings)\n",
    "\n",
    "    return normalized_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity search functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def normalize(vectors: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Normalizes a set of vectors so that each vector has a magnitude (or length) of 1.\n",
    "\n",
    "    Parameters:\n",
    "    vectors (np.ndarray): A 2D NumPy array where each row is a vector to be normalized.\n",
    "\n",
    "    Returns:\n",
    "    np.ndarray: A 2D NumPy array of the same shape where each vector has been normalized to have a unit length.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate the norms (magnitudes) of each vector along the rows (axis=1)\n",
    "    norms = np.linalg.norm(vectors, axis=1, keepdims=True)\n",
    "    \n",
    "    # Normalize each vector by dividing by its corresponding norm\n",
    "    return vectors / norms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cosine_similarity(v1: np.ndarray, v2: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Computes the cosine similarity between two vectors.\n",
    "\n",
    "    Cosine similarity is a measure of similarity between two vectors of an inner product space \n",
    "    that measures the cosine of the angle between them. The value ranges from -1 to 1, where:\n",
    "    - 1 indicates that the two vectors are identical in direction,\n",
    "    - 0 indicates that the vectors are orthogonal (no similarity),\n",
    "    - -1 indicates that the vectors are diametrically opposed.\n",
    "\n",
    "    Parameters:\n",
    "    v1 (np.ndarray): The first vector.\n",
    "    v2 (np.ndarray): The second vector.\n",
    "\n",
    "    Returns:\n",
    "    float: The cosine similarity between `v1` and `v2`.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate the dot product of the two vectors\n",
    "    dot_product = np.dot(v1, v2)\n",
    "    \n",
    "    # Calculate the norms (magnitudes) of both vectors\n",
    "    norm_v1 = np.linalg.norm(v1)\n",
    "    norm_v2 = np.linalg.norm(v2)\n",
    "    \n",
    "    # Compute the cosine similarity\n",
    "    return dot_product / (norm_v1 * norm_v2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def find_nearest_neighbors_faiss(normalized_embeddings: np.ndarray, unit_vector: np.ndarray, k: int = 4) -> tuple:\n",
    "    \"\"\"\n",
    "    Creates a FAISS index using L2 distance (which acts like cosine similarity for normalized vectors), \n",
    "    adds normalized embeddings to the index, and finds the k nearest neighbors to a query vector.\n",
    "\n",
    "    Parameters:\n",
    "    normalized_embeddings (np.ndarray): A 2D array of normalized embeddings to be added to the FAISS index.\n",
    "    unit_vector (np.ndarray): The query vector, which should also be normalized and reshaped to (1, dimension).\n",
    "    k (int): The number of nearest neighbors to find. Default is 4.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing two elements:\n",
    "        - indices (np.ndarray): The indices of the k nearest neighbors in the embedding space.\n",
    "        - distances (np.ndarray): The corresponding distances to the nearest neighbors.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Get the dimensionality of the embeddings\n",
    "    dimension = normalized_embeddings.shape[1]\n",
    "    \n",
    "    # Step 2: Create a FAISS index based on L2 distance\n",
    "    index = faiss.IndexFlatL2(dimension)\n",
    "    \n",
    "    # Step 3: Add normalized embeddings to the FAISS index\n",
    "    index.add(normalized_embeddings)\n",
    "    \n",
    "    # Step 4: Ensure the query vector is reshaped to (1, dimension)\n",
    "    query_vector = unit_vector.reshape(1, -1)\n",
    "    \n",
    "    # Step 5: Perform the search for the k nearest neighbors\n",
    "    distances, indices = index.search(query_vector, k)\n",
    "    \n",
    "    # Return the indices of the nearest neighbors and their corresponding distances\n",
    "    return indices, distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Response functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt_completion(model: str, prompt: str) -> dict:\n",
    "    \"\"\"\n",
    "    Sends a prompt to the GPT model and retrieves the completion.\n",
    "\n",
    "    Args:\n",
    "        model (str): The model to be used, such as \"gpt-4\".\n",
    "        prompt (str): The prompt or system message to be sent to the GPT model.\n",
    "\n",
    "    Returns:\n",
    "        dict: The response from the GPT model, containing the completion.\n",
    "    \"\"\"\n",
    "    chat = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": prompt,\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    return chat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_system_prompt(user_prompt: str, selected_professionals_string: str):\n",
    "    system_prompt = f\"\"\" You are a chatbot for a staff enhancement company. \n",
    "        Your goal is to assist employees in finding professionals inside the company who can help them if they have questions.\n",
    "        \n",
    "        The user query in this case is: {user_prompt}\n",
    "        \n",
    "        And you have the next list of experts who can help him:\n",
    "        {selected_professionals_string}.\n",
    "\n",
    "        Answer cordially with the name of the experts and a brief description of the their expertise and technology knowledge. \n",
    "        In the end, make a subjective evaluation of the expert, and say how much do you think he/she can help\n",
    "    \"\"\"\n",
    "    return system_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to classify the user's intent using GPT directly\n",
    "def classify_user_intent_with_gpt(user_prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Uses the GPT model to classify the user's intent.\n",
    "    Returns \"search expert\" or \"general inquiry\" depending on the user's query.\n",
    "    \"\"\"\n",
    "    classification_prompt = f\"\"\"You are a helpful assistant. The user has asked: '{user_prompt}'.\n",
    "    \n",
    "    Please classify this query as one of the following:\n",
    "    1. \"search expert\" (The user is looking for an expert to assist them with a specific topic.)\n",
    "    2. \"general inquiry\" (The user is asking a general question that doesn't require a company expert.)\n",
    "    \n",
    "    Respond only with one of the two options: \"search expert\" or \"general inquiry\".\n",
    "    \"\"\"\n",
    "\n",
    "    # Call to GPT to classify the intent\n",
    "    classification_response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are an assistant that classifies user queries.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": classification_prompt\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Extract the classification from the model\n",
    "    intent = classification_response.choices[0].message.content\n",
    "    \n",
    "    #print(intent)\n",
    "    return intent\n",
    "\n",
    "def build_general_prompt(user_prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Builds the system prompt for general inquiries.\n",
    "    \"\"\"\n",
    "    system_prompt = f\"\"\"You are a helpful assistant. The user asked: {user_prompt}. \n",
    "        Please respond with a useful and informative answer.\"\"\"\n",
    "    return system_prompt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate embeddings for the user's prompt\n",
    "def generate_user_embeddings(user_prompt: str) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generates embeddings for the user prompt using the specified model.\n",
    "    \"\"\"\n",
    "    embedding_user = client.embeddings.create(\n",
    "        input=user_prompt,\n",
    "        model=\"text-embedding-ada-002\"\n",
    "    )\n",
    "    return np.array(embedding_user.data[0].embedding)\n",
    "\n",
    "# Function to find the nearest neighbors using FAISS\n",
    "def find_similar_experts(embedding_user_array: np.ndarray, k: int = 4):\n",
    "    \"\"\"\n",
    "    Finds the top k closest experts based on the user's embeddings.\n",
    "    \"\"\"\n",
    "    # Load precomputed normalized embeddings\n",
    "    normalized_embeddings = np.load(\"normalized_embeddings.npy\")\n",
    "    \n",
    "    # Find the nearest neighbors using FAISS\n",
    "    indices, distances = find_nearest_neighbors_faiss(normalized_embeddings, embedding_user_array, k=k)\n",
    "    \n",
    "    return indices, distances\n",
    "\n",
    "# Function to select the most similar expert profiles\n",
    "def select_professionals(indices: np.ndarray, profile_df: pd.DataFrame) -> str:\n",
    "    \"\"\"\n",
    "    Selects the profiles of the most similar experts based on the provided indices.\n",
    "    \"\"\"\n",
    "    selected_professionals = []\n",
    "    \n",
    "    # Append the profiles of the most similar professionals\n",
    "    for n in indices[0]:\n",
    "        selected_professionals.append(create_profile_prompt(profile_df.iloc[n]))\n",
    "\n",
    "    # Convert the list into a single string, separated by new lines\n",
    "    return \"\\n\".join(selected_professionals)\n",
    "\n",
    "# Function to handle the expert search logic\n",
    "def handle_expert_search(user_prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Handles the complete expert search logic based on the user prompt.\n",
    "    \"\"\"\n",
    "    # Generate embeddings for the user prompt\n",
    "    embedding_user_array = generate_user_embeddings(user_prompt)\n",
    "    \n",
    "    # Find the nearest experts using FAISS\n",
    "    indices, distances = find_similar_experts(embedding_user_array)\n",
    "    \n",
    "    # Load professional profiles directly here\n",
    "    profile_df = pd.read_csv(\"experts_profile.csv\")\n",
    "    \n",
    "    # Select the profiles of the most similar experts\n",
    "    selected_professionals_string = select_professionals(indices, profile_df)\n",
    "    \n",
    "    # Build the system prompt by combining the user prompt and the selected expert profiles\n",
    "    system_prompt = f\"\"\" You are a chatbot for a staff enhancement company. \n",
    "        Your goal is to assist employees in finding professionals inside the company who can help them if they have questions.\n",
    "        \n",
    "        The user query in this case is: {user_prompt}\n",
    "        \n",
    "        And you have the next list of experts who can help him:\n",
    "        {selected_professionals_string}.\n",
    "\n",
    "        Answer cordially with the name of the experts and a brief description of their expertise and technology knowledge. \n",
    "        In the end, make a subjective evaluation of the expert, and say how much do you think he/she can help.\n",
    "    \"\"\"\n",
    "    \n",
    "    return system_prompt\n",
    "\n",
    "# Main function to generate the response\n",
    "def generate_response(user_prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Generates the complete response depending on whether the user is looking for an expert or has a general inquiry.\n",
    "    \"\"\"\n",
    "    # Classify the user's intent using GPT directly\n",
    "    user_intent = classify_user_intent_with_gpt(user_prompt)\n",
    "\n",
    "    if user_intent == \"search expert\":\n",
    "        print(\"Wait a few moments while I search the database\")\n",
    "        print(\"This shouldn't take more than 20 seconds\")\n",
    "        print()\n",
    "        \n",
    "        # Handle expert search\n",
    "        expert_search_prompt = handle_expert_search(user_prompt)\n",
    "        \n",
    "        # Generate the response using the GPT-4 model\n",
    "        expert_helper_chat = gpt_completion(\"gpt-4\", expert_search_prompt)\n",
    "        \n",
    "        return expert_helper_chat.choices[0].message.content\n",
    "    \n",
    "    else:\n",
    "        # If it's a general inquiry, we construct a generic prompt\n",
    "        system_prompt = build_general_prompt(user_prompt)\n",
    "\n",
    "        # Generate the response using the GPT-4 model\n",
    "        standard_chat = gpt_completion(\"gpt-4\", system_prompt)\n",
    "        \n",
    "        return standard_chat.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
